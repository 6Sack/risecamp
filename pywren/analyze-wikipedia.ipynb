{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyWren RISECamp, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analytics with PyWren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will use PyWren explore a dataset of Wikipedia records.\n",
    "\n",
    "## 0. The Data\n",
    "\n",
    "We've prepared an S3 bucket with 20GB of Wikipedia traffic statistics data obtained from http://aws.amazon.com/datasets/4182. To make the analysis more feasible for the short time you're here, we've shortened the dataset to three days worth of data (May 5 to May 7, 2009; roughly 20G and 329 million entries). \n",
    "\n",
    "Let's take a look into the bucket with our dataset. We'll print a few files from a few files from our bucket.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from training import list_keys_with_prefix, read_from_s3, wikipedia_bucket\n",
    "\n",
    "filenames = list_keys_with_prefix(wikipedia_bucket, \"wikistats_20090505_restricted-01/\")\n",
    "for filename in filenames[:20]:\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 74 files (2 of which are intentionally left empty). Each file consists of a list of records. Let's go take a look into the first file. This may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read a file into memory, and split on newlines.\n",
    "records = read_from_s3(wikipedia_bucket, wikistats_20090505_restricted-01/part-00001).split(\"\\n\")\n",
    "\n",
    "print(\"The total number of records in this file is {}, but here are the first 20\".format(len(records)))\n",
    "for i in range(20):\n",
    "    print records[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each record has stats for a single Wikipedia page. The schema is:\n",
    "\n",
    "`<date_time> <project_code> <page_name> <page_views> <page_size>`\n",
    "\n",
    "- `<date_time>` specifies a date in YYYYMMDD-HHmmSS format (year, month, day, hour minute, second).\n",
    "- `<project_code>` specifies the language the page is written in. For example, project code “en” indicates an English page. \n",
    "- `<page_title>` gives the page title.\n",
    "- `<page_views>` gives the number of page views in the hour-long time slot starting at `<data_time>`. \n",
    "- `<page_size>` gives the size in bytes of the  page.\n",
    "\n",
    "Now that we have a better understanding of the structure of our data, we can start running some interesting queries. \n",
    "\n",
    "Because the data are so large, it takes us quite a while to load just one file, as we saw in the previous exercise. You could imagine it would take way too long to process all of the data sequentially. Thankfully, we have [PyWren](https://www.youtube.com/watch?v=VSaDPc1Cs5U)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. To the ~~Batmobile~~ Cloud!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some libraries that are useful for this tutorial\n",
    "import sys\n",
    "\n",
    "# We need to load PyWren and create an executor instance\n",
    "import pywren\n",
    "pwex = pywren.default_executor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Count\n",
    "Let’s see how many records in total are in this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count(filename):\n",
    "    data = read_from_s3(wikipedia_bucket, filename)\n",
    "    return (len(data.split(\"\\n\")) if data else 0)    \n",
    "\n",
    "futures = pwex.map(count, filenames)\n",
    "pywren.wait(futures)\n",
    "\n",
    "result = sum([f.result() for f in futures])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should launch 73 PyWren tasks. After finishing the job, let's plot again to check the execution. This should look more interesting than the simple job before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_pywren_execution(futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Visits for English Pages\n",
    "Recall from above when we peek the date, that the second field is the “project code” and contains information about the language of the pages. For example, the project code “en” indicates an English page. Let’s calculate the page counts of english pages, grouped by dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from functools import reduce\n",
    "\n",
    "def aggregate_count(key_value_list):\n",
    "    def reduce_f(obj1, obj2):\n",
    "        return(obj1[0], obj1[1] + obj2[1])\n",
    "    counts = [reduce(reduce_f, group) for _, group \n",
    "          in groupby(sorted(key_value_list), key=itemgetter(0))]\n",
    "    \n",
    "    return counts\n",
    "\n",
    "def english_page_count(filename):\n",
    "    data = read_from_s3(wikipedia_bucket, filename)\n",
    "    # filter out the english pages\n",
    "    en_pages = [d for d in data.split(\"\\n\") \n",
    "                if len(d.split(\" \")) >= 4 and d.split(\" \")[1] == \"en\"]\n",
    "    # projection to create (date, pagecount) pairs\n",
    "    en_kvpair_list = [(p.split(\" \")[0][:8], int(p.split(\" \")[3])) for p in en_pages]\n",
    "\n",
    "    return aggregate_count(en_kvpair_list)\n",
    "    \n",
    "futures = pwex.map(english_page_count, filenames)\n",
    "pywren.wait(futures)\n",
    "\n",
    "results = [f.result() for f in futures]\n",
    "en_page_counts_by_date = aggregate_count([x for y in results for x in y])\n",
    "print(en_page_counts_by_date)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
