{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Exercise 6 - Training with Ray and Serving with Clipper\n",
    "\n",
    "**GOAL:** The goal of this exercise is to show how to train a policy with Ray and to deploy it with Clipper in a fun, interactive way.\n",
    "\n",
    "We will train an agent to play Pong, and then we will play Pong against the policy that we trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import gym\n",
    "import pong_py\n",
    "import ray\n",
    "\n",
    "from ray.rllib.ppo import PPOAgent, DEFAULT_CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Ray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for redis server at 127.0.0.1:49181 to respond...\n",
      "Waiting for redis server at 127.0.0.1:55556 to respond...\n",
      "Starting local scheduler with 4 CPUs, 0 GPUs\n",
      "View the web UI at http://localhost:8889/notebooks/ray_ui25886.ipynb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'local_scheduler_socket_names': ['/tmp/scheduler85656775'],\n",
       " 'node_ip_address': '127.0.0.1',\n",
       " 'object_store_addresses': [ObjectStoreAddress(name='/tmp/plasma_store7176638', manager_name='/tmp/plasma_manager26229005', manager_port=56579)],\n",
       " 'redis_address': '127.0.0.1:49181'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below is a hack. The explanation is as follows. Internally within the `PPOAgent` constructor, a number of actors are created, and these actors will instantiate gym environments using the command `gym.make('PongJS-v0')`. The command `gym.make` knows how to instantiate a number of pre-defined environments that are shipped with the `gym` module. However, the `PongJS-v0` environment is defined in the `pong_py` module and is registered with the `gym` module when the `import pong_py` statement gets run.\n",
    "\n",
    "Therefore, for the actors to successfully instantiate the gym environments, the `pong_py` module must be imported on the actors. This is why we define a remote function `import_pong_py` which closes over the `pong_py` environment. When the actors are created, that remote function is unpickled on the actors which forces the `pong_py` module to be imported, which enables the `gym` module to create the `PongJS-v0` environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is a hack.\n",
    "@ray.remote\n",
    "def import_pong_py():\n",
    "    pong_py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate an agent that can be trained using Proximal Policy Optimization (PPO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-31 02:26:00,350] PPOAgent algorithm created with logdir '/tmp/ray/PongJS-v0_PPOAgent_2017-08-31_02-26-00Q8I340'\n",
      "[2017-08-31 02:26:00,352] Making new env: PongJS-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-atari env, not using any observation preprocessor.\n",
      "Constructing fcnet [32, 32] <function tanh at 0x114b75398>\n",
      "Constructing fcnet [32, 32] <function tanh at 0x114b75398>\n",
      "Constructing fcnet [32, 32] <function tanh at 0x114b75398>\n",
      "Constructing fcnet [32, 32] <function tanh at 0x114b75398>\n",
      "Constructing fcnet [32, 32] <function tanh at 0x114b75398>\n",
      "Constructing fcnet [32, 32] <function tanh at 0x114b75398>\n",
      "Constructing fcnet [32, 32] <function tanh at 0x114b75398>\n",
      "Constructing fcnet [32, 32] <function tanh at 0x114b75398>\n",
      "Constructing fcnet [32, 32] <function tanh at 0x114b75398>\n",
      "Constructing fcnet [32, 32] <function tanh at 0x114b75398>\n"
     ]
    }
   ],
   "source": [
    "config = DEFAULT_CONFIG.copy()\n",
    "config['num_workers'] = 3\n",
    "config['num_sgd_iter'] = 20\n",
    "config['sgd_batchsize'] = 8196\n",
    "config['model']['fcnet_hiddens'] = [32, 32]\n",
    "\n",
    "agent = PPOAgent('PongJS-v0', config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the `PPOAgent` for some number of iterations.\n",
    "\n",
    "**EXERCISE:** You will need to experiment with the number of iterations as well as with the configuration to get the agent to learn something reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> iteration 1\n",
      "total reward is  153.595505618\n",
      "trajectory length mean is  152.595505618\n",
      "timesteps: 40743\n",
      "Computing policy (iterations=20, stepsize=5e-05):\n",
      "           iter     total loss    policy loss        vf loss             kl        entropy\n",
      "              0    5.18152e+03   -1.92394e-03    5.18152e+03    4.12476e-08    1.09860e+00\n",
      "              1    5.18144e+03   -1.93869e-03    5.18145e+03    3.17493e-07    1.09860e+00\n",
      "              2    5.18136e+03   -1.95562e-03    5.18137e+03    1.01237e-06    1.09860e+00\n",
      "              3    5.18128e+03   -1.96921e-03    5.18128e+03    2.17570e-06    1.09860e+00\n",
      "              4    5.18120e+03   -1.98596e-03    5.18120e+03    3.76647e-06    1.09860e+00\n",
      "              5    5.18112e+03   -2.00005e-03    5.18112e+03    5.78781e-06    1.09859e+00\n",
      "              6    5.18104e+03   -2.01340e-03    5.18104e+03    8.29189e-06    1.09859e+00\n",
      "              7    5.18095e+03   -2.02955e-03    5.18095e+03    1.12526e-05    1.09859e+00\n",
      "              8    5.18087e+03   -2.04442e-03    5.18087e+03    1.45851e-05    1.09858e+00\n",
      "              9    5.18078e+03   -2.06016e-03    5.18078e+03    1.82941e-05    1.09858e+00\n",
      "             10    5.18069e+03   -2.07301e-03    5.18069e+03    2.25384e-05    1.09857e+00\n",
      "             11    5.18060e+03   -2.08858e-03    5.18060e+03    2.72701e-05    1.09857e+00\n",
      "             12    5.18051e+03   -2.10477e-03    5.18051e+03    3.22740e-05    1.09856e+00\n",
      "             13    5.18041e+03   -2.11951e-03    5.18042e+03    3.76628e-05    1.09855e+00\n",
      "             14    5.18032e+03   -2.13146e-03    5.18032e+03    4.38115e-05    1.09855e+00\n",
      "             15    5.18022e+03   -2.14995e-03    5.18022e+03    5.02633e-05    1.09854e+00\n",
      "             16    5.18012e+03   -2.16449e-03    5.18012e+03    5.69427e-05    1.09853e+00\n",
      "             17    5.18001e+03   -2.17827e-03    5.18002e+03    6.42530e-05    1.09852e+00\n",
      "             18    5.17991e+03   -2.19460e-03    5.17991e+03    7.22111e-05    1.09852e+00\n",
      "             19    5.17980e+03   -2.20792e-03    5.17980e+03    8.05440e-05    1.09851e+00\n",
      "kl div: 8.0544e-05\n",
      "kl coeff: 0.1\n",
      "rollouts time: 16.4505050182\n",
      "shuffle time: 0.00732803344727\n",
      "load time: 0.0181670188904\n",
      "sgd time: 2.31139826775\n",
      "sgd examples/s: 17626.9925303\n",
      "total time so far: 27.471613884\n",
      "===> iteration 2\n",
      "total reward is  155.496240602\n",
      "trajectory length mean is  154.496240602\n",
      "timesteps: 41096\n",
      "Computing policy (iterations=20, stepsize=5e-05):\n",
      "           iter     total loss    policy loss        vf loss             kl        entropy\n",
      "              0    5.33509e+03   -3.84878e-04    5.33509e+03    8.36292e-08    1.09850e+00\n",
      "              1    5.33494e+03   -3.95558e-04    5.33494e+03    6.14462e-07    1.09849e+00\n",
      "              2    5.33478e+03   -4.08497e-04    5.33478e+03    1.73032e-06    1.09848e+00\n",
      "              3    5.33462e+03   -4.21248e-04    5.33462e+03    3.41841e-06    1.09847e+00\n",
      "              4    5.33446e+03   -4.33645e-04    5.33446e+03    5.90248e-06    1.09846e+00\n",
      "              5    5.33429e+03   -4.45395e-04    5.33429e+03    9.61935e-06    1.09845e+00\n",
      "              6    5.33411e+03   -4.62034e-04    5.33411e+03    1.40235e-05    1.09843e+00\n",
      "              7    5.33393e+03   -4.77484e-04    5.33393e+03    1.89109e-05    1.09842e+00\n",
      "              8    5.33375e+03   -4.92511e-04    5.33375e+03    2.45386e-05    1.09841e+00\n",
      "              9    5.33356e+03   -5.06531e-04    5.33357e+03    3.09137e-05    1.09840e+00\n",
      "             10    5.33337e+03   -5.18871e-04    5.33337e+03    3.81937e-05    1.09838e+00\n",
      "             11    5.33317e+03   -5.35173e-04    5.33317e+03    4.61040e-05    1.09837e+00\n",
      "             12    5.33297e+03   -5.48661e-04    5.33297e+03    5.45899e-05    1.09835e+00\n",
      "             13    5.33276e+03   -5.62115e-04    5.33276e+03    6.43881e-05    1.09834e+00\n",
      "             14    5.33255e+03   -5.76608e-04    5.33255e+03    7.53845e-05    1.09832e+00\n",
      "             15    5.33233e+03   -5.90949e-04    5.33233e+03    8.70485e-05    1.09830e+00\n",
      "             16    5.33211e+03   -6.05473e-04    5.33211e+03    9.96736e-05    1.09828e+00\n",
      "             17    5.33188e+03   -6.21762e-04    5.33188e+03    1.12138e-04    1.09826e+00\n",
      "             18    5.33164e+03   -6.32526e-04    5.33165e+03    1.25954e-04    1.09824e+00\n",
      "             19    5.33140e+03   -6.50138e-04    5.33140e+03    1.40562e-04    1.09822e+00\n",
      "kl div: 0.000140562\n",
      "kl coeff: 0.05\n",
      "rollouts time: 16.5867340565\n",
      "shuffle time: 0.00960397720337\n",
      "load time: 0.00244998931885\n",
      "sgd time: 3.21722507477\n",
      "sgd examples/s: 12773.7410486\n",
      "total time so far: 47.2944209576\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    result = agent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the agent manually by calling `agent.compute_action` and see the rewards you get are consistent with the rewards printed during the training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('PongJS-v0')\n",
    "\n",
    "for _ in range(20):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    cumulative_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.compute_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        cumulative_reward += reward\n",
    "\n",
    "    print(cumulative_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint the agent so that the relevant model can be saved and deployed to Clipper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:/tmp/ray/PongJS-v0_PPOAgent_2017-08-31_02-26-00Q8I340/checkpoint-2 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-31 02:26:53,866] /tmp/ray/PongJS-v0_PPOAgent_2017-08-31_02-26-00Q8I340/checkpoint-2 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = agent.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play Against the Policy\n",
    "\n",
    "In this section, we will play Pong against the policy that we just trained. The game will be played in your browser, and the policy that we trained will be served by Clipper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Deploy your policy using Clipper. Follow the instructions that get printed below to play Pong against the deployed policy. You'll need to deploy all of the data that is saved in the directory `os.path.dirname(checkpoint_path)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by importing the `clipper_admin` library and use that to create a new Clipper instance to serve the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-31 02:32:48,941] Starting managed Redis instance in Docker\n",
      "[2017-08-31 02:32:51,060] Clipper is running\n"
     ]
    }
   ],
   "source": [
    "# Make logging work correctly in the Jupyter notebook\n",
    "import logging\n",
    "import sys\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "from clipper_admin import DockerContainerManager, ClipperConnection\n",
    "clipper_conn = ClipperConnection(DockerContainerManager())\n",
    "clipper_conn.start_clipper(query_frontend_image=\"clipper/query_frontend:cors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, deploy the saved policy checkpoint to Clipper. The policy will run in a Docker container we created for this exercise.\n",
    "\n",
    "**TODO(crankshaw):** link to model container code once it's on github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-31 02:32:57,841] Found existing Dockerfile in /tmp/ray/PongJS-v0_PPOAgent_2017-08-31_02-26-00Q8I340. This file will be overwritten\n",
      "[2017-08-31 02:32:57,845] Building model Docker image with model data from /tmp/ray/PongJS-v0_PPOAgent_2017-08-31_02-26-00Q8I340\n",
      "[2017-08-31 02:32:58,265] Pushing model Docker image to pong-policy:1\n",
      "[2017-08-31 02:32:59,693] Found 0 replicas for pong-policy:1. Adding 1\n",
      "[2017-08-31 02:33:00,370] Successfully registered model pong-policy:1\n",
      "[2017-08-31 02:33:00,371] Done deploying model pong-policy:1.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "model_name = \"pong-policy\"\n",
    "app_name = \"pong\"\n",
    "clipper_conn.build_and_deploy_model(\n",
    "    name=model_name,\n",
    "    version=1,\n",
    "    input_type=\"doubles\",\n",
    "    model_data_path=os.path.dirname(checkpoint_path),\n",
    "    base_image=\"clipper/risecamp-pong-container\",\n",
    "    force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, register a Clipper application and link it the deployed policy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-31 02:33:04,402] Application pong was successfully registered\n",
      "[2017-08-31 02:33:04,417] Model pong-policy is now linked to application pong\n"
     ]
    }
   ],
   "source": [
    "app_name = \"pong\"\n",
    "clipper_conn.register_application(name=app_name, default_output=\"0\", input_type=\"doubles\", slo_micros=100000)\n",
    "clipper_conn.link_model_to_app(app_name=app_name, model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can play Pong against the policy in the browser.\n",
    "\n",
    "**TODO(crankshaw):** Once the Javascript pong works, print out address they need to copy and paste to direct the JS to the right Clipper instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'localhost:1337'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clipper_conn.get_query_addr()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
